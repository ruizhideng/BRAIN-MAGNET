{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd807c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logomaker\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Sequential, Conv2d, BatchNorm2d, BatchNorm1d, ReLU, MaxPool2d, Flatten, Linear, Dropout, MSELoss\n",
    "from  torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# training device: gpu or cpu\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/scratch/rdeng/enhancer_project/ipython_notebooks/')\n",
    "from helper import IOHelper, SequenceHelper \n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The device is {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct CNN model\n",
    "\n",
    "class CNN_STARR(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN model: 4 convolution layers followed with two linear layers. forward two heads.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN_STARR, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            Conv2d(4, 128, (1,11), padding=\"same\"),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(),\n",
    "            MaxPool2d((1,2), (1,2)),\n",
    "            Conv2d(128, 256, (1,9), padding=\"same\"),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(),\n",
    "            MaxPool2d((1,2), (1,2)),\n",
    "            Conv2d(256, 512, (1,7), padding=\"same\"),\n",
    "            BatchNorm2d(512),\n",
    "            ReLU(),\n",
    "           MaxPool2d((1,2), (1,2)),\n",
    "#             Conv1d(512, 1024, 3, padding=1),\n",
    "#             BatchNorm1d(1024),\n",
    "#             ReLU(),\n",
    "#             MaxPool1d(2),\n",
    "            Flatten(),\n",
    "            Linear(64000, 1024),\n",
    "            BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Dropout(0.4),\n",
    "            Linear(1024, 1024),\n",
    "            BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Dropout(0.4),\n",
    "            Linear(1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "cnn_starr = CNN_STARR()\n",
    "cnn_starr.to(device)\n",
    "\n",
    "print(cnn_starr)\n",
    "summary(cnn_starr, input_size = (4, 1, 1000), batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b104c8",
   "metadata": {},
   "source": [
    "### Approach 1: calculate contribution score on Test data; don't include augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to convert only one piece of sequence to np.array\n",
    "# from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
    "\n",
    "# def one_hot_encode(sequence: str,\n",
    "#                    alphabet: str = 'ACGT',\n",
    "#                    neutral_alphabet: str = 'N',\n",
    "#                    neutral_value: Any = 0,\n",
    "#                    dtype=np.float32) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     One-hot encode for a sequence.\n",
    "#     A -> [1,0,0,0]\n",
    "#     C -> [0,1,0,0]\n",
    "#     G -> [0,0,1,0]\n",
    "#     T -> [0,0,0,1]\n",
    "#     N -> [0,0,0,0]\n",
    "#     \"\"\"\n",
    "#     def to_uint8(string):\n",
    "#         return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
    "#     hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
    "#     hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "#     hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
    "#     hash_table = hash_table.astype(dtype)\n",
    "#     return hash_table[to_uint8(sequence)]\n",
    "\n",
    "\n",
    "# # function to generate all sequences to np.array (one-hot encoding matrix)\n",
    "# def generate_sequence_matrix(input_fasta):\n",
    "#     \"\"\"\n",
    "#     After the function of one_hot_encode,\n",
    "#     convert the whole dateset to one_hot encoding matrix\n",
    "#     \"\"\"\n",
    "#     sequence_matrix = []\n",
    "#     for i in range(0,len(input_fasta.sequence)):\n",
    "#         snippet = one_hot_encode(input_fasta.sequence[i])\n",
    "\n",
    "#         arr_pre = 1000\n",
    "#         arr_len = len(snippet)\n",
    "#         arr_int = int((arr_pre-arr_len)/2)\n",
    "#         arr_ceil = math.ceil((arr_pre-arr_len)/2)\n",
    "        \n",
    "#         snippet=np.pad(snippet, [(arr_int, arr_ceil), (0, 0)], mode='constant')\n",
    "#         sequence_matrix.append(snippet)\n",
    "        \n",
    "#     sequence_matrix=np.array(sequence_matrix)\n",
    "#     return sequence_matrix\n",
    "\n",
    "\n",
    "# # convert np.array to tensor\n",
    "# def prepare_tensor(X_test, Y_test):\n",
    "#     tensor_x = torch.Tensor(X_test).permute(0,2,1).unsqueeze(2) # convert input: [batch, 1000, 4] -> [batch, 4, 1, 1000] \n",
    "#     tensor_y = torch.Tensor(Y_test).unsqueeze(1) # convert targets: [2, batch] -> [batch, 2], make targets like classification task\n",
    "#     tensor_data = TensorDataset(tensor_x, tensor_y)\n",
    "#     return tensor_data\n",
    "\n",
    "# # function to load sequences and enhancer activity, don't use augment data\n",
    "# def create_dataset(batch_size, task):\n",
    "#     # Read whole fasta files\n",
    "# #     file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/Enhancer.fa\")\n",
    "#     file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/train_set/Sequences_Test.fa\")\n",
    "\n",
    "#     input_fasta = IOHelper.get_fastas_from_file(file_seq, uppercase=True)\n",
    "\n",
    "#     # Convert sequence to one hot encoding matrix\n",
    "#     seq_matrix = generate_sequence_matrix(input_fasta[:14812])\n",
    "\n",
    "#     X = np.nan_to_num(seq_matrix) # Replace NaN with zero and infinity with large finite numbers\n",
    "#     X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "# #     Activity = pd.read_table(\"/data/scratch/rdeng/enhancer_project/data/Enhancer_activity.txt\")\n",
    "#     Activity = pd.read_table(\"/data/scratch/rdeng/enhancer_project/data/train_set/Sequences_activity_Test.txt\")\n",
    "\n",
    "#     Y_NSC = Activity[:14812].NSC_log2_enrichment\n",
    "#     Y_ESC = Activity[:14812].ESC_log2_enrichment\n",
    "    \n",
    "#     if task == \"NSC\":\n",
    "#         Y = Y_NSC\n",
    "#     elif task == \"ESC\":\n",
    "#         Y = Y_ESC\n",
    "#     else:\n",
    "#         print(\"Please provide a correct cell type\")\n",
    "#     tensor_data = prepare_tensor(X_reshaped, Y)\n",
    "#     tensor_size = len(tensor_data)\n",
    "#     tensor_dataloader = DataLoader(tensor_data, batch_size)\n",
    "#     print(\"the {} data size is {}\".format(set, len(tensor_data)))\n",
    "\n",
    "#     return tensor_dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e845df",
   "metadata": {},
   "source": [
    "### Approach 2: calculate contribution score on highly active enhancers (Top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert only one piece of sequence to np.array\n",
    "from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
    "\n",
    "def one_hot_encode(sequence: str,\n",
    "                   alphabet: str = 'ACGT',\n",
    "                   neutral_alphabet: str = 'N',\n",
    "                   neutral_value: Any = 0,\n",
    "                   dtype=np.float32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    One-hot encode for a sequence.\n",
    "    A -> [1,0,0,0]\n",
    "    C -> [0,1,0,0]\n",
    "    G -> [0,0,1,0]\n",
    "    T -> [0,0,0,1]\n",
    "    N -> [0,0,0,0]\n",
    "    \"\"\"\n",
    "    def to_uint8(string):\n",
    "        return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
    "    hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
    "    hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "    hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
    "    hash_table = hash_table.astype(dtype)\n",
    "    return hash_table[to_uint8(sequence)]\n",
    "\n",
    "\n",
    "# function to generate all sequences to np.array (one-hot encoding matrix)\n",
    "def generate_sequence_matrix(input_fasta):\n",
    "    \"\"\"\n",
    "    After the function of one_hot_encode,\n",
    "    convert the whole dateset to one_hot encoding matrix\n",
    "    \"\"\"\n",
    "    sequence_matrix = []\n",
    "    for i in range(0,len(input_fasta.sequence)):\n",
    "        snippet = one_hot_encode(input_fasta.sequence[i])\n",
    "\n",
    "        arr_pre = 1000\n",
    "        arr_len = len(snippet)\n",
    "        arr_int = int((arr_pre-arr_len)/2)\n",
    "        arr_ceil = math.ceil((arr_pre-arr_len)/2)\n",
    "        # padding the fasta with 0, if the length < 1000\n",
    "        snippet=np.pad(snippet, [(arr_int, arr_ceil), (0, 0)], mode='constant')\n",
    "        sequence_matrix.append(snippet)\n",
    "        \n",
    "    sequence_matrix=np.array(sequence_matrix)\n",
    "    return sequence_matrix\n",
    "\n",
    "\n",
    "# convert np.array to tensor\n",
    "def prepare_tensor(X_test, Y_test):\n",
    "    tensor_x = torch.Tensor(X_test).permute(0,2,1).unsqueeze(2) # convert input: [batch, 1000, 4] -> [batch, 4, 1, 1000] \n",
    "    tensor_y = torch.Tensor(Y_test).unsqueeze(1) # convert targets: [2, batch] -> [batch, 2], make targets like classification task\n",
    "    tensor_data = TensorDataset(tensor_x, tensor_y)\n",
    "    return tensor_data\n",
    "\n",
    "# function to load sequences and enhancer activity, don't use augment data\n",
    "def create_dataset(batch_size, task):\n",
    "    \n",
    "    # Read fasta files of Test dataset\n",
    "    file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/Enhancer.fa\")\n",
    "    input_fasta = IOHelper.get_fastas_from_file(file_seq, uppercase=True)  \n",
    "    # Don't include augment fasta: Reversed\n",
    "    input_fasta = input_fasta[~input_fasta[\"location\"].str.contains(\"Reversed\")]\n",
    "    \n",
    "    # Load activity table\n",
    "    Activity = pd.read_table(\"/data/scratch/rdeng/enhancer_project/data/Enhancer_activity.txt\")\n",
    "    # Don't include augment activity: Reversed\n",
    "    Activity = Activity.iloc[input_fasta.index]\n",
    "\n",
    "    # Keep Top10 highly active enhancers\n",
    "    NSC_top10 = 1.652084635\n",
    "    ESC_top10 = 1.820433242\n",
    "    \n",
    "    if task == \"NSC\":\n",
    "        Activity_NSC = Activity[Activity.NSC_log2_enrichment >= NSC_top10]\n",
    "        Y = Activity_NSC.NSC_log2_enrichment\n",
    "    elif task == \"ESC\":\n",
    "        Activity_ESC = Activity[Activity.ESC_log2_enrichment >= ESC_top10]\n",
    "        Y = Activity_ESC.ESC_log2_enrichment\n",
    "    else:\n",
    "        print(\"Please provide a correct cell type\")\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    # select the Top10 fasta\n",
    "    input_fasta = input_fasta.iloc[Y.index]\n",
    "    input_fasta = input_fasta.reset_index(drop=True) # reset the index otherwise the generate_sequence_matric doesn't work\n",
    "    seq_matrix = generate_sequence_matrix(input_fasta)\n",
    "        \n",
    "    X = np.nan_to_num(seq_matrix) # Replace NaN with zero and infinity with large finite numbers\n",
    "    X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Prepapre the tensor input  \n",
    "    Y = Y.reset_index(drop=True)  #reset the index otherwise the prepare_tensor doesn't work\n",
    "    tensor_data = prepare_tensor(X_reshaped, Y)\n",
    "    tensor_size = len(tensor_data)\n",
    "    tensor_dataloader = DataLoader(tensor_data, batch_size)\n",
    "    print(\"the {} data size is {}\".format(set, len(tensor_data)))\n",
    "\n",
    "    return tensor_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035efe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(task):\n",
    "    \n",
    "    # Read fasta files of Test dataset\n",
    "    file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/Enhancer.fa\")\n",
    "    input_fasta = IOHelper.get_fastas_from_file(file_seq, uppercase=True)  \n",
    "    # Don't include augment fasta: Reversed\n",
    "    input_fasta = input_fasta[~input_fasta[\"location\"].str.contains(\"Reversed\")]\n",
    "    \n",
    "    # Load activity table\n",
    "    Activity = pd.read_table(\"/data/scratch/rdeng/enhancer_project/data/Enhancer_activity.txt\")\n",
    "    # Don't include augment activity: Reversed\n",
    "    Activity = Activity.iloc[input_fasta.index]\n",
    "\n",
    "    # Keep Top10 highly active enhancers\n",
    "    NSC_top10 = 1.652084635\n",
    "    ESC_top10 = 1.820433242\n",
    "    \n",
    "    if task == \"NSC\":\n",
    "        Activity_NSC = Activity[Activity.NSC_log2_enrichment >= NSC_top10]\n",
    "        Y = Activity_NSC.NSC_log2_enrichment\n",
    "    elif task == \"ESC\":\n",
    "        Activity_ESC = Activity[Activity.ESC_log2_enrichment >= ESC_top10]\n",
    "        Y = Activity_ESC.ESC_log2_enrichment\n",
    "    else:\n",
    "        print(\"Please provide a correct cell type\")\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    # select the Top10 fasta\n",
    "    input_fasta = input_fasta.iloc[Y.index]\n",
    "    input_fasta = input_fasta.reset_index(drop=True) # reset the index otherwise the generate_sequence_matric doesn't work\n",
    "    \n",
    "    input_fasta.to_csv(f'/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/High_table_' + task + '.txt', index=False, header=True, sep='\\t')\n",
    "\n",
    "save_dataset(\"NSC\")\n",
    "save_dataset(\"ESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca2e97",
   "metadata": {},
   "source": [
    "### Approach 3: calculate contribution score on highly active enhancers (Top10) of Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ad2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to convert only one piece of sequence to np.array\n",
    "# from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
    "\n",
    "# def one_hot_encode(sequence: str,\n",
    "#                    alphabet: str = 'ACGT',\n",
    "#                    neutral_alphabet: str = 'N',\n",
    "#                    neutral_value: Any = 0,\n",
    "#                    dtype=np.float32) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     One-hot encode for a sequence.\n",
    "#     A -> [1,0,0,0]\n",
    "#     C -> [0,1,0,0]\n",
    "#     G -> [0,0,1,0]\n",
    "#     T -> [0,0,0,1]\n",
    "#     N -> [0,0,0,0]\n",
    "#     \"\"\"\n",
    "#     def to_uint8(string):\n",
    "#         return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
    "#     hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
    "#     hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "#     hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
    "#     hash_table = hash_table.astype(dtype)\n",
    "#     return hash_table[to_uint8(sequence)]\n",
    "\n",
    "\n",
    "# # function to generate all sequences to np.array (one-hot encoding matrix)\n",
    "# def generate_sequence_matrix(input_fasta):\n",
    "#     \"\"\"\n",
    "#     After the function of one_hot_encode,\n",
    "#     convert the whole dateset to one_hot encoding matrix\n",
    "#     \"\"\"\n",
    "#     sequence_matrix = []\n",
    "#     for i in range(0,len(input_fasta.sequence)):\n",
    "#         snippet = one_hot_encode(input_fasta.sequence[i])\n",
    "\n",
    "#         arr_pre = 1000\n",
    "#         arr_len = len(snippet)\n",
    "#         arr_int = int((arr_pre-arr_len)/2)\n",
    "#         arr_ceil = math.ceil((arr_pre-arr_len)/2)\n",
    "#         # padding the fasta with 0, if the length < 1000\n",
    "#         snippet=np.pad(snippet, [(arr_int, arr_ceil), (0, 0)], mode='constant')\n",
    "#         sequence_matrix.append(snippet)\n",
    "        \n",
    "#     sequence_matrix=np.array(sequence_matrix)\n",
    "#     return sequence_matrix\n",
    "\n",
    "\n",
    "# # convert np.array to tensor\n",
    "# def prepare_tensor(X_test, Y_test):\n",
    "#     tensor_x = torch.Tensor(X_test).permute(0,2,1).unsqueeze(2) # convert input: [batch, 1000, 4] -> [batch, 4, 1, 1000] \n",
    "#     tensor_y = torch.Tensor(Y_test).unsqueeze(1) # convert targets: [2, batch] -> [batch, 2], make targets like classification task\n",
    "#     tensor_data = TensorDataset(tensor_x, tensor_y)\n",
    "#     return tensor_data\n",
    "\n",
    "# # function to load sequences and enhancer activity, don't use augment data\n",
    "# def create_dataset(batch_size, task):\n",
    "    \n",
    "#     # Read fasta files of Test dataset\n",
    "#     file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/train_set/Sequences_Test.fa\")\n",
    "#     input_fasta = IOHelper.get_fastas_from_file(file_seq, uppercase=True)  \n",
    "#     # Don't include augment fasta: Reversed\n",
    "#     input_fasta = input_fasta[~input_fasta[\"location\"].str.contains(\"Reversed\")]\n",
    "    \n",
    "#     # Load activity table\n",
    "#     Activity = pd.read_table(\"/data/scratch/rdeng/enhancer_project/data/train_set/Sequences_activity_Test.txt\")\n",
    "#     # Don't include augment activity: Reversed\n",
    "#     Activity = Activity.iloc[input_fasta.index]\n",
    "\n",
    "#     # Keep Top10 highly active enhancers\n",
    "#     NSC_top10 = 1.652084635\n",
    "#     ESC_top10 = 1.820433242\n",
    "    \n",
    "#     if task == \"NSC\":\n",
    "#         Activity_NSC = Activity[Activity.NSC_log2_enrichment >= NSC_top10]\n",
    "#         Y = Activity_NSC.NSC_log2_enrichment\n",
    "#     elif task == \"ESC\":\n",
    "#         Activity_ESC = Activity[Activity.ESC_log2_enrichment >= ESC_top10]\n",
    "#         Y = Activity_ESC.ESC_log2_enrichment\n",
    "#     else:\n",
    "#         print(\"Please provide a correct cell type\")\n",
    "\n",
    "#     # Convert sequence to one hot encoding matrix\n",
    "#     # select the Top10 fasta\n",
    "#     input_fasta = input_fasta.iloc[Y.index]\n",
    "#     input_fasta = input_fasta.reset_index(drop=True) # reset the index otherwise the generate_sequence_matric doesn't work\n",
    "#     seq_matrix = generate_sequence_matrix(input_fasta)\n",
    "        \n",
    "#     X = np.nan_to_num(seq_matrix) # Replace NaN with zero and infinity with large finite numbers\n",
    "#     X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "    \n",
    "#     # Prepapre the tensor input  \n",
    "#     Y = Y.reset_index(drop=True)  #reset the index otherwise the prepare_tensor doesn't work\n",
    "#     tensor_data = prepare_tensor(X_reshaped, Y)\n",
    "#     tensor_size = len(tensor_data)\n",
    "#     tensor_dataloader = DataLoader(tensor_data, batch_size)\n",
    "#     print(\"the {} data size is {}\".format(set, len(tensor_data)))\n",
    "\n",
    "#     return tensor_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c5a38",
   "metadata": {},
   "source": [
    "### Approach 4: calculate contribution score on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to convert only one piece of sequence to np.array\n",
    "# from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
    "\n",
    "# def one_hot_encode(sequence: str,\n",
    "#                    alphabet: str = 'ACGT',\n",
    "#                    neutral_alphabet: str = 'N',\n",
    "#                    neutral_value: Any = 0,\n",
    "#                    dtype=np.float32) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     One-hot encode for a sequence.\n",
    "#     A -> [1,0,0,0]\n",
    "#     C -> [0,1,0,0]\n",
    "#     G -> [0,0,1,0]\n",
    "#     T -> [0,0,0,1]\n",
    "#     N -> [0,0,0,0]\n",
    "#     \"\"\"\n",
    "#     def to_uint8(string):\n",
    "#         return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
    "#     hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
    "#     hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "#     hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
    "#     hash_table = hash_table.astype(dtype)\n",
    "#     return hash_table[to_uint8(sequence)]\n",
    "\n",
    "\n",
    "# # function to generate all sequences to np.array (one-hot encoding matrix)\n",
    "# def generate_sequence_matrix(input_fasta):\n",
    "#     \"\"\"\n",
    "#     After the function of one_hot_encode,\n",
    "#     convert the whole dateset to one_hot encoding matrix\n",
    "#     \"\"\"\n",
    "#     sequence_matrix = []\n",
    "#     for i in range(0,len(input_fasta.sequence)):\n",
    "#         snippet = one_hot_encode(input_fasta.sequence[i])\n",
    "\n",
    "#         arr_pre = 1000\n",
    "#         arr_len = len(snippet)\n",
    "#         arr_int = int((arr_pre-arr_len)/2)\n",
    "#         arr_ceil = math.ceil((arr_pre-arr_len)/2)\n",
    "#         # padding the fasta with 0, if the length < 1000\n",
    "#         snippet=np.pad(snippet, [(arr_int, arr_ceil), (0, 0)], mode='constant')\n",
    "#         sequence_matrix.append(snippet)\n",
    "        \n",
    "#     sequence_matrix=np.array(sequence_matrix)\n",
    "#     return sequence_matrix\n",
    "\n",
    "\n",
    "# # convert np.array to tensor\n",
    "# def prepare_tensor(X_test, Y_test):\n",
    "#     tensor_x = torch.Tensor(X_test).permute(0,2,1).unsqueeze(2) # convert input: [batch, 1000, 4] -> [batch, 4, 1, 1000] \n",
    "#     tensor_y = torch.Tensor(Y_test).unsqueeze(1) # convert targets: [2, batch] -> [batch, 2], make targets like classification task\n",
    "#     tensor_data = TensorDataset(tensor_x, tensor_y)\n",
    "#     return tensor_data\n",
    "\n",
    "# # function to load sequences and enhancer activity, don't use augment data\n",
    "# def create_dataset(batch_size, task):\n",
    "    \n",
    "#     # Read fasta files of Test dataset\n",
    "#     file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/Enhancer.fa\")\n",
    "#     input_fasta = IOHelper.get_fastas_from_file(file_seq, uppercase=True)  \n",
    "#     # Don't include augment fasta: Reversed\n",
    "#     input_fasta = input_fasta[~input_fasta[\"location\"].str.contains(\"Reversed\")]\n",
    "    \n",
    "#     # Load activity table\n",
    "#     Activity = pd.read_table(\"/data/scratch/rdeng/enhancer_project/data/Enhancer_activity.txt\")\n",
    "#     # Don't include augment activity: Reversed\n",
    "#     Activity = Activity.iloc[input_fasta.index]\n",
    "    \n",
    "#     if task == \"NSC\":\n",
    "#         Y = Activity.NSC_log2_enrichment\n",
    "#     elif task == \"ESC\":\n",
    "#         Y = Activity.ESC_log2_enrichment\n",
    "#     else:\n",
    "#         print(\"Please provide a correct cell type\")\n",
    "\n",
    "#     # Convert sequence to one hot encoding matrix\n",
    "#     # select the Top10 fasta\n",
    "#     input_fasta = input_fasta.iloc[Y.index]\n",
    "#     input_fasta = input_fasta.reset_index(drop=True) # reset the index otherwise the generate_sequence_matric doesn't work\n",
    "#     seq_matrix = generate_sequence_matrix(input_fasta)\n",
    "        \n",
    "#     X = np.nan_to_num(seq_matrix) # Replace NaN with zero and infinity with large finite numbers\n",
    "#     X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "    \n",
    "#     # Prepapre the tensor input  \n",
    "#     Y = Y.reset_index(drop=True)  #reset the index otherwise the prepare_tensor doesn't work\n",
    "#     tensor_data = prepare_tensor(X_reshaped, Y)\n",
    "#     tensor_size = len(tensor_data)\n",
    "#     tensor_dataloader = DataLoader(tensor_data, batch_size)\n",
    "#     print(\"the {} data size is {}\".format(set, len(tensor_data)))\n",
    "\n",
    "#     return tensor_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f7f14",
   "metadata": {},
   "source": [
    "### calculate contribution score, GradientShap with gradient correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT MESSAGE!!! In order to run the code on the GPU, I changed the \"/trinity/home/rdeng/enhancer/lib/python3.7/site-packages/torch/_tensor.py\"\n",
    "### return self.numpy() -> return self.cpu().detach().numpy() \n",
    "### return self.numpy().astype(dtype, copy=False) -> return self.cpu().detach().numpy().astype(dtype, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.visualization import viz_sequence\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import shap\n",
    "import importlib\n",
    "from importlib import reload\n",
    "reload(shap.explainers.deep)\n",
    "reload(shap.explainers.deep.deep_pytorch)\n",
    "reload(shap.explainers)\n",
    "reload(shap)\n",
    "import torch\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle, traverse_edges, shuffle_edges, prepare_edges\n",
    "\n",
    "#this function performs a dinucleotide shuffle of a one-hot encoded sequence\n",
    "#It expects the supplied input in the format (length x 4)\n",
    "def onehot_dinuc_shuffle(s):\n",
    "    s = np.squeeze(s)\n",
    "    assert len(s.shape)==2\n",
    "    assert s.shape[1]==4\n",
    "    argmax_vals = \"\".join([str(x) for x in np.argmax(s, axis=-1)])\n",
    "    shuffled_argmax_vals = [int(x) for x in traverse_edges(argmax_vals,                          \n",
    "                            shuffle_edges(prepare_edges(argmax_vals)))]\n",
    "    to_return = np.zeros_like(s)    \n",
    "    to_return[list(range(len(s))), shuffled_argmax_vals] = 1\n",
    "    return to_return\n",
    "\n",
    "#This generates 100 dinuc-shuffled references per sequence\n",
    "# 100 references per sequence is on the high side; around 10 work well in practice\n",
    "# I am using 100 references here just for demonstration purposes.\n",
    "#Note that when an input of None is supplied, the function returns a tensor\n",
    "# that has the same dimensions as actual input batches\n",
    "def shuffle_several_times(inp):\n",
    "    #I am assuming len(inp) == 1 because this function is designed for models with one\n",
    "    # input mode (i.e. just sequence as the input mode)\n",
    "    assert (inp is None) or len(inp)==1\n",
    "    if (inp is None):\n",
    "        return torch.tensor(np.zeros((1,4,1,1000)).astype(\"float32\")).to(device)\n",
    "    else:\n",
    "        #Some reshaping/transposing needs to be performed before calling\n",
    "        # onehot_dinuc_shuffle becuase the input to the DeepSEA model\n",
    "        # is in the format (4 x 1 x length) for each sequence, whereas\n",
    "        # onehot_dinuc_shuffle expects (length x 4)\n",
    "        to_return = torch.tensor(np.array([\n",
    "            onehot_dinuc_shuffle(\n",
    "                inp[0].detach().cpu().numpy().squeeze().transpose(1,0)).transpose((1,0))[:,None,:]\n",
    "            for i in range(100)]).astype(\"float32\")).to(device)\n",
    "        return to_return\n",
    "    \n",
    "    \n",
    "#This combine_mult_and_diffref function can be used to generate hypothetical\n",
    "# importance scores for one-hot encoded sequence.\n",
    "#Hypothetical scores can be thought of as quick estimates of what the\n",
    "# contribution *would have been* if a different base were present. Hypothetical\n",
    "# scores are used as input to the importance score clustering algorithm\n",
    "# TF-MoDISco (https://github.com/kundajelab/tfmodisco)\n",
    "# Hypothetical importance scores are discussed more in this pull request:\n",
    "#  https://github.com/kundajelab/deeplift/pull/36\n",
    "def combine_mult_and_diffref(mult, orig_inp, bg_data):\n",
    "    to_return = []\n",
    "    #Perform some reshaping/transposing because the code was designed\n",
    "    # for inputs that are in the format (length x 4), whereas the DeepSEA\n",
    "    # model has inputs in the format (4 x 1 x length)\n",
    "    mult = [x.squeeze().transpose((0,2,1)) for x in mult]\n",
    "    orig_inp = [x.squeeze().transpose((1,0)) for x in orig_inp]\n",
    "    bg_data = [x.squeeze().transpose((0,2,1)) for x in bg_data]\n",
    "    for l in range(len(mult)):\n",
    "        #At each position in the input sequence, we iterate over the one-hot encoding\n",
    "        # possibilities (eg: for genomic sequence, this is ACGT i.e.\n",
    "        # 1000, 0100, 0010 and 0001) and compute the hypothetical \n",
    "        # difference-from-reference in each case. We then multiply the hypothetical\n",
    "        # differences-from-reference with the multipliers to get the hypothetical contributions.\n",
    "        #For each of the one-hot encoding possibilities,\n",
    "        # the hypothetical contributions are then summed across the ACGT axis to estimate\n",
    "        # the total hypothetical contribution of each position. This per-position hypothetical\n",
    "        # contribution is then assigned (\"projected\") onto whichever base was present in the\n",
    "        # hypothetical sequence.\n",
    "        #The reason this is a fast estimate of what the importance scores *would* look\n",
    "        # like if different bases were present in the underlying sequence is that\n",
    "        # the multipliers are computed once using the original sequence, and are not\n",
    "        # computed again for each hypothetical sequence.\n",
    "        projected_hypothetical_contribs = np.zeros_like(bg_data[l]).astype(\"float\")\n",
    "        assert len(orig_inp[l].shape)==2, orig_inp[l].shape\n",
    "        for i in range(orig_inp[l].shape[-1]):\n",
    "            hypothetical_input = np.zeros_like(orig_inp[l]).astype(\"float\")\n",
    "            hypothetical_input[:,i] = 1.0\n",
    "            hypothetical_difference_from_reference = (hypothetical_input[None,:,:]-bg_data[l])\n",
    "            hypothetical_contribs = hypothetical_difference_from_reference*mult[l]\n",
    "            projected_hypothetical_contribs[:,:,i] = np.sum(hypothetical_contribs,axis=-1) \n",
    "        to_return.append(torch.tensor(np.mean(projected_hypothetical_contribs,axis=0).transpose((1,0))[:,None,:]).to(device))\n",
    "    return to_return\n",
    "\n",
    "\n",
    "\n",
    "def load_model(task):\n",
    "    if task == \"NSC\":\n",
    "        # cnn_starr.load_state_dict(torch.load(\"/data/scratch/rdeng/enhancer_project/model/checkpoint_NSC_212697.2D.pth\", map_location=torch.device('cpu')))\n",
    "        cnn_starr.load_state_dict(torch.load(\"/data/scratch/rdeng/enhancer_project/model/checkpoint_NSC_212697.2D.pth\"))\n",
    "    elif task == \"ESC\":\n",
    "#         cnn_starr.load_state_dict(torch.load(\"/data/scratch/rdeng/enhancer_project/model/checkpoint_ESC_212696.2D.pth\", map_location=torch.device('cpu')))\n",
    "        cnn_starr.load_state_dict(torch.load(\"/data/scratch/rdeng/enhancer_project/model/checkpoint_ESC_212696.2D.pth\"))\n",
    "    else:\n",
    "        print(\"Please provide correct cell type\")\n",
    "    return cnn_starr\n",
    "    \n",
    "    \n",
    "# calculate contribution score, and save it as npy for TF-modisco\n",
    "def contri_score(dataloader, task):\n",
    "    \n",
    "    whole_inputs = []\n",
    "    whole_shap_explanations = []\n",
    "\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # calculate shap\n",
    "        e = shap.DeepExplainer((cnn_starr), shuffle_several_times,\n",
    "                       combine_mult_and_diffref=combine_mult_and_diffref)\n",
    "\n",
    "        shap_explanations = e.shap_values(inputs)\n",
    "        \n",
    "        # shap_explanations = shap_explanations.cpu().detach().numpy()\n",
    "\n",
    "        # process gradients with gradient correction (Majdandzic et al. 2022)\n",
    "        inputs = inputs.cpu().detach().numpy()\n",
    "\n",
    "        whole_inputs.append(inputs)\n",
    "        whole_shap_explanations.append(shap_explanations)\n",
    "\n",
    "    whole_inputs = np.concatenate(whole_inputs, axis=0)\n",
    "    whole_inputs = whole_inputs.squeeze() # remove additional dimention for TFmodisco, (Batch, 4, 1, 1000)->(Batch, 4, 1000)\n",
    "    whole_shap_explanations = np.concatenate(whole_shap_explanations, axis=0)\n",
    "    whole_shap_explanations = whole_shap_explanations.squeeze() # remove additional dimention for TFmodisco, (Batch, 4, 1, 1000)->(Batch, 4, 1000)\n",
    "    \n",
    "    np.save('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/shap_explanations_'+ task + '.npy', whole_shap_explanations)\n",
    "    np.save('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/inp_'+ task + '.npy', whole_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate contribution score\n",
    "\n",
    "# NSC\n",
    "task = \"NSC\"\n",
    "test_dataloader_NSC = create_dataset(5, task = task)\n",
    "load_model(task)\n",
    "contri_score(test_dataloader_NSC, task = task)\n",
    "test = contri_score(test_dataloader_NSC, task = task)\n",
    "\n",
    "# ESC\n",
    "# task = \"ESC\"\n",
    "# test_dataloader_ESC = create_dataset(5, task = task)\n",
    "# load_model(task)\n",
    "# contri_score(test_dataloader_ESC, task = task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046a7b6",
   "metadata": {},
   "source": [
    "### percentile contribution score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6e10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the contribution scores and input sequences\n",
    "NSC_contri = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/Whole/shap_explanations_NSC.npy')\n",
    "NSC_inp = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/Whole/inp_NSC.npy')\n",
    "NSC_actural = np.multiply(NSC_contri, NSC_inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fadc4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fasta files of Test dataset\n",
    "file_seq = str(\"/data/scratch/rdeng/enhancer_project/data/Enhancer.fa\")\n",
    "input_fasta = IOHelper.get_fastas_from_file(file_seq, uppercase=True)  \n",
    "# Don't include augment fasta: Reversed\n",
    "input_fasta = input_fasta[~input_fasta[\"location\"].str.contains(\"Reversed\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "df34de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chr</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Enhancer</th>\n",
       "      <th>Length</th>\n",
       "      <th>Contri_score</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>191043</td>\n",
       "      <td>chr1:191042-192042</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.909272e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13513379</th>\n",
       "      <td>chr10</td>\n",
       "      <td>82578397</td>\n",
       "      <td>chr10:82577398-82578398</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.909272e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101133102</th>\n",
       "      <td>chr8</td>\n",
       "      <td>24698675</td>\n",
       "      <td>chr8:24698674-24699674</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.909272e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34377521</th>\n",
       "      <td>chr15</td>\n",
       "      <td>40520681</td>\n",
       "      <td>chr15:40519682-40520682</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.909272e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34377522</th>\n",
       "      <td>chr15</td>\n",
       "      <td>40520682</td>\n",
       "      <td>chr15:40519682-40520682</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.909272e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74872450</th>\n",
       "      <td>chr3</td>\n",
       "      <td>146582312</td>\n",
       "      <td>chr3:146581874-146582385</td>\n",
       "      <td>511</td>\n",
       "      <td>0.493953</td>\n",
       "      <td>112242616.0</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000442</th>\n",
       "      <td>chr8</td>\n",
       "      <td>142815844</td>\n",
       "      <td>chr8:142815290-142816085</td>\n",
       "      <td>795</td>\n",
       "      <td>0.525803</td>\n",
       "      <td>112242617.0</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93738399</th>\n",
       "      <td>chr6</td>\n",
       "      <td>150857350</td>\n",
       "      <td>chr6:150857177-150857722</td>\n",
       "      <td>545</td>\n",
       "      <td>0.549321</td>\n",
       "      <td>112242618.0</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93738402</th>\n",
       "      <td>chr6</td>\n",
       "      <td>150857353</td>\n",
       "      <td>chr6:150857177-150857722</td>\n",
       "      <td>545</td>\n",
       "      <td>0.573878</td>\n",
       "      <td>112242619.0</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93738409</th>\n",
       "      <td>chr6</td>\n",
       "      <td>150857360</td>\n",
       "      <td>chr6:150857177-150857722</td>\n",
       "      <td>545</td>\n",
       "      <td>0.611389</td>\n",
       "      <td>112242620.0</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112242620 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Chr        Pos                  Enhancer  Length Contri_score  \\\n",
       "0           chr1     191043        chr1:191042-192042    1000          0.0   \n",
       "13513379   chr10   82578397   chr10:82577398-82578398    1000          0.0   \n",
       "101133102   chr8   24698675    chr8:24698674-24699674    1000          0.0   \n",
       "34377521   chr15   40520681   chr15:40519682-40520682    1000          0.0   \n",
       "34377522   chr15   40520682   chr15:40519682-40520682    1000          0.0   \n",
       "...          ...        ...                       ...     ...          ...   \n",
       "74872450    chr3  146582312  chr3:146581874-146582385     511     0.493953   \n",
       "105000442   chr8  142815844  chr8:142815290-142816085     795     0.525803   \n",
       "93738399    chr6  150857350  chr6:150857177-150857722     545     0.549321   \n",
       "93738402    chr6  150857353  chr6:150857177-150857722     545     0.573878   \n",
       "93738409    chr6  150857360  chr6:150857177-150857722     545     0.611389   \n",
       "\n",
       "                  Rank    Percentile  \n",
       "0                  1.0  8.909272e-07  \n",
       "13513379           1.0  8.909272e-07  \n",
       "101133102          1.0  8.909272e-07  \n",
       "34377521           1.0  8.909272e-07  \n",
       "34377522           1.0  8.909272e-07  \n",
       "...                ...           ...  \n",
       "74872450   112242616.0  1.000000e+02  \n",
       "105000442  112242617.0  1.000000e+02  \n",
       "93738399   112242618.0  1.000000e+02  \n",
       "93738402   112242619.0  1.000000e+02  \n",
       "93738409   112242620.0  1.000000e+02  \n",
       "\n",
       "[112242620 rows x 7 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Your genome coordinates\n",
    "# coordinates = input_fasta.location[:20] # time-consuming, test 20 enhancers!!!\n",
    "coordinates = input_fasta.location\n",
    "\n",
    "# Function to extract start and end coordinates from a string\n",
    "def extract_coordinates(coord_string):\n",
    "    chr_, coordinates_str = coord_string.split(\":\")\n",
    "    start, end = map(int, coordinates_str.split(\"-\"))\n",
    "    return chr_, start, end\n",
    "\n",
    "# Create a DataFrame with the original coordinates\n",
    "df = pd.DataFrame({'Enhancer': coordinates})\n",
    "\n",
    "# Extract chromosome, start, and end coordinates\n",
    "df[['Chr', 'Start', 'End']] = df['Enhancer'].apply(lambda x: pd.Series(extract_coordinates(x)))\n",
    "\n",
    "# Add a new column with consecutive numbers for each range starting from start+1\n",
    "df['Pos'] = df.apply(lambda row: list(range(row['Start'] + 1, row['End'] + 1)), axis=1)\n",
    "df['Length'] = df['End'] - df['Start']\n",
    "\n",
    "###################\n",
    "# Your numpy data\n",
    "# numpy_data = NSC_actural[:20,::] # time-consuming, test 20 enhancers!!!\n",
    "numpy_data = NSC_actural\n",
    "\n",
    "# Lengths\n",
    "# lengths = df.Length[:20] # time-consuming, test 20 enhancers!!!\n",
    "lengths = df.Length\n",
    "\n",
    "result = []\n",
    "\n",
    "for i, length in enumerate(lengths):\n",
    "    arr_pre = 1000\n",
    "    arr_len = length\n",
    "    arr_int = int((arr_pre - arr_len) / 2)\n",
    "    arr_end = arr_int + length\n",
    "\n",
    "    # Extract subarray based on arr_int and arr_ceil\n",
    "    subarray = numpy_data[i, :, arr_int:arr_end]\n",
    "    max_num = np.abs(np.sum(subarray, axis = 0))\n",
    "\n",
    "    result.append(max_num)\n",
    "###################\n",
    "\n",
    "\n",
    "df['Contri_score'] = result\n",
    "\n",
    "# Explode the list of consecutive numbers into separate rows\n",
    "df = df.explode(['Pos', 'Contri_score']).reset_index(drop=True)\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['Chr', 'Pos', 'Enhancer', 'Length', 'Contri_score']]\n",
    "df\n",
    "\n",
    "# Calculate percentiles for Contri_score\n",
    "# Rank Contri_score with handling ties by using the 'min' method\n",
    "df['Rank'] = df['Contri_score'].rank(method='min') \n",
    "\n",
    "# Calculate percentiles\n",
    "df['Percentile'] = df['Rank'] / len(df) * 100\n",
    "\n",
    "\n",
    "\n",
    "df = df.sort_values(by=['Contri_score'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c543fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Whole_contri_percentile.txt\", index = False, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1abb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNP info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a383b9e",
   "metadata": {},
   "source": [
    "### visualize sequence with motifs of NSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f585add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NSC_top10 = 1.652084635\n",
    "ESC_top10 = 1.820433242\n",
    "\n",
    "# load preds and targets data\n",
    "preds = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/preds_targets/preds_NSC_High.npy')\n",
    "\n",
    "targets = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/preds_targets/targets_NSC_High.npy')\n",
    "\n",
    "# Find indices of elements greater than NSC_top10 in both arrays\n",
    "precise_idx = np.where(np.logical_and(preds > NSC_top10, targets > NSC_top10))[0]\n",
    "# load contribution score from Test data\n",
    "contri_scores = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/shap_explanations_NSC.npy')\n",
    "inps = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/inp_NSC.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db763831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/10000/out_NSC.h5\"\n",
    "f = h5py.File(filename, \"r\")\n",
    "\n",
    "# in total three layers of the h5\n",
    "print(list(f['pos_patterns']))\n",
    "print(list(f['pos_patterns']['pattern_0']))\n",
    "print(list(f['pos_patterns']['pattern_0']['seqlets']))\n",
    "\n",
    "motif_idx = f['pos_patterns']['pattern_1']['seqlets']['example_idx'][()]\n",
    "selected_idx = np.intersect1d(precise_idx, motif_idx)\n",
    "print(\"The selected idx for TP53 is {}\".format(selected_idx))\n",
    "\n",
    "motif_idx = f['pos_patterns']['pattern_6']['seqlets']['example_idx'][()]\n",
    "selected_idx = np.intersect1d(precise_idx, motif_idx)\n",
    "print(\"The selected idx for YY2 is {}\".format(selected_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of YY2 (MA0748.2), 470:481\n",
    "# example of TP53 (MA0106.3), 579:600\n",
    "# the hypothetical_contribs\n",
    "mod_viz = contri_scores[13776]\n",
    "viz_sequence.plot_weights(mod_viz[:,:], subticks_frequency=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual contribs_scores\n",
    "mod_viz = np.multiply(contri_scores[13776], inps[13776])\n",
    "viz_sequence.plot_weights(mod_viz[:,:], subticks_frequency=100, \n",
    "                          highlight={'red': ([470,481],[579,596])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr8:123479279-123480279\n",
    "print(preds[13776])\n",
    "print(targets[13776])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911730c",
   "metadata": {},
   "source": [
    "### visualize sequence with motifs of ESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1278499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NSC_top10 = 1.652084635\n",
    "ESC_top10 = 1.820433242\n",
    "\n",
    "# load preds and targets data\n",
    "preds = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/preds_targets/preds_ESC_High.npy')\n",
    "\n",
    "targets = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/preds_targets/targets_ESC_High.npy')\n",
    "\n",
    "# Find indices of elements greater than NSC_top10 in both arrays\n",
    "precise_idx = np.where(np.logical_and(preds > ESC_top10, targets > ESC_top10))[0]\n",
    "# load contribution score from Test data\n",
    "contri_scores = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/shap_explanations_ESC.npy')\n",
    "inps = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/inp_ESC.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70754ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/10000/out_ESC.h5\"\n",
    "f = h5py.File(filename, \"r\")\n",
    "\n",
    "motif_idx = f['neg_patterns']['pattern_0']['seqlets']['example_idx'][()]\n",
    "selected_idx1 = np.intersect1d(precise_idx, motif_idx)\n",
    "print(\"The selected idx for ZKSCAN5 is {}\".format(selected_idx1))\n",
    "\n",
    "motif_idx = f['neg_patterns']['pattern_2']['seqlets']['example_idx'][()]\n",
    "selected_idx2 = np.intersect1d(precise_idx, motif_idx)\n",
    "print(\"The selected idx for Pou5f1::Sox2 is {}\".format(selected_idx2))\n",
    "\n",
    "motif_idx = f['neg_patterns']['pattern_7']['seqlets']['example_idx'][()]\n",
    "selected_idx3 = np.intersect1d(precise_idx, motif_idx)\n",
    "print(\"The selected idx for Pou5f1::Sox2 is {}\".format(selected_idx3))\n",
    "np.intersect1d(selected_idx1, selected_idx3)\n",
    "\n",
    "# I visualized 6511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr2:121490069-121490672\n",
    "print(preds[6511])\n",
    "print(targets[6511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa91ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "High_table_NSC = pd.read_csv('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/High_table_NSC.txt', sep= '\\t')\n",
    "print(High_table_NSC.iloc[13776, 0])\n",
    "print(len(High_table_NSC.iloc[13776, 1]))\n",
    "\n",
    "High_table_ESC = pd.read_csv('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/High_table_ESC.txt', sep= '\\t')\n",
    "print(High_table_ESC.iloc[6511, 0])\n",
    "print(len(High_table_ESC.iloc[6511, 1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5bcecb",
   "metadata": {},
   "source": [
    "### example_idx to extract the genomic coordinates for closest gene expression and epigenome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c023016",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/10000/out_NSC.h5\"\n",
    "f = h5py.File(filename, \"r\")\n",
    "\n",
    "patterns = f['pos_patterns']\n",
    "pattern_names = list(patterns.keys())\n",
    "\n",
    "data = []\n",
    "for pattern_name in pattern_names:\n",
    "    seqlets = patterns[pattern_name]['seqlets']\n",
    "    example_idx = seqlets['example_idx'][()]\n",
    "    for idx in example_idx:\n",
    "        data.append((pattern_name, idx))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['pattern_name', 'example_idx'])\n",
    "High_table_NSC = pd.read_csv('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/High_table_NSC.txt', sep= '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4517cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/10000/out_NSC.h5\"\n",
    "f = h5py.File(filename, \"r\")\n",
    "\n",
    "data = []\n",
    "\n",
    "If \n",
    "for pattern_type in ['pos_patterns', 'neg_patterns']:\n",
    "    patterns = f[pattern_type]\n",
    "    pattern_names = list(patterns.keys())\n",
    "    for pattern_name in pattern_names:\n",
    "        seqlets = patterns[pattern_name]['seqlets']\n",
    "        example_idx = seqlets['example_idx'][()]\n",
    "        for idx in example_idx:\n",
    "            data.append((pattern_type, pattern_name, idx))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['pattern_type', 'pattern_name', 'example_idx'])\n",
    "High_table_NSC = pd.read_csv('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/High_table_NSC.txt', sep= '\\t')\n",
    "\n",
    "# merge example_idx with index of genomic coordinates\n",
    "merged_df = pd.merge(df, High_table_NSC, left_on='example_idx', right_index=True)\n",
    "merged_df[['pattern_type', 'pattern_name', 'location']].to_csv(f'/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/patterns_coordinates_NSC.txt', index=False, header=True, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32351e3d",
   "metadata": {},
   "source": [
    "### Trial trim_zero after getting the contribution scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "contri_scores = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/shap_explanations_NSC.npy')\n",
    "inps = np.load('/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/inp_NSC.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_idx(inps):\n",
    "    \"\"\"\n",
    "    Return the index of padded zero of DNA sequences\n",
    "    inps:  onehot coded matrix: [num, 4, length]\n",
    "    \"\"\"\n",
    "    inp_len = inps.shape[2]\n",
    "    first_nonzero = []\n",
    "    for num in range(inps.shape[0]): \n",
    "        for i in range(inps[num].shape[1]):\n",
    "            if inps[num][:, :i+1].sum() != 0:\n",
    "                break\n",
    "        first_nonzero.append(i)\n",
    "\n",
    "    last_nonzero = []\n",
    "    for num in range(inps.shape[0]): \n",
    "        if inps[num][:, inp_len-1:].sum() != 0:\n",
    "            last_nonzero.append(inp_len)\n",
    "        else:\n",
    "            for i in (range(inps[num].shape[1])):\n",
    "                if inps[num][:, -i-1:-1].sum() != 0:\n",
    "                    break\n",
    "            last_nonzero.append(inp_len - i)\n",
    "    return zip(first_nonzero, last_nonzero)\n",
    "\n",
    "def unpadded(idx_nonzero, inps):\n",
    "    \"\"\"\n",
    "    Return unpadded arrary based on the padded_idx function\n",
    "    idx_zero: the resulr from padded_index, zip(first_nonzero, last_nonzero)\n",
    "    inps:  onehot coded matrix: [num, 4, length]\n",
    "    \n",
    "    \"\"\"\n",
    "    non_padded = []\n",
    "    for num, (first_nonzero, last_nonzero) in enumerate(idx_nonzero):\n",
    "#         first_nonzero, last_nonzero = 0, 1000 # IMPORTANT: this won't trim zeros\n",
    "        num_unpadded = inps[num, :, first_nonzero:last_nonzero]\n",
    "        num_unpadded = num_unpadded.transpose(1,0) # convert [4,length] -> [length, 4] for the TFmodisco\n",
    "        non_padded.append(num_unpadded.astype('float32'))\n",
    "    return non_padded\n",
    "                                                        \n",
    "\n",
    "inps_unpad = unpadded(padded_idx(inps) ,inps)\n",
    "hypoth_unpad = unpadded(padded_idx(inps) ,contri_scores)                                                    \n",
    "\n",
    "# create an empty list to store the result\n",
    "contri_unpad = []\n",
    "# iterate through each array in the lists and multiply the elements\n",
    "for i in range(len(hypoth_unpad)):\n",
    "    temp = np.multiply(hypoth_unpad[i], inps_unpad[i])\n",
    "    contri_unpad.append(temp)\n",
    "\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# hypoth_unpad = OrderedDict([('task0', hypoth_unpad)])\n",
    "# contri_unpad = OrderedDict([('task0', contri_unpad)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import modisco\n",
    "\n",
    "#Uncomment to refresh modules for when tweaking code during development:\n",
    "from importlib import reload\n",
    "reload(modisco.util)\n",
    "reload(modisco.pattern_filterer)\n",
    "reload(modisco.aggregator)\n",
    "reload(modisco.core)\n",
    "reload(modisco.seqlet_embedding.advanced_gapped_kmer)\n",
    "reload(modisco.affinitymat.transformers)\n",
    "reload(modisco.affinitymat.core)\n",
    "reload(modisco.affinitymat)\n",
    "reload(modisco.cluster.core)\n",
    "reload(modisco.cluster)\n",
    "reload(modisco.tfmodisco_workflow.seqlets_to_patterns)\n",
    "reload(modisco.tfmodisco_workflow)\n",
    "reload(modisco)\n",
    "\n",
    "null_per_pos_scores = modisco.coordproducers.LaplaceNullDist(num_to_samp=5000)\n",
    "tfmodisco_results = modisco.tfmodisco_workflow.workflow.TfModiscoWorkflow(\n",
    "                    #Slight modifications from the default settings\n",
    "    sliding_window_size=15,\n",
    "    flank_size=5,\n",
    "    target_seqlet_fdr=0.15,\n",
    "    seqlets_to_patterns_factory=\n",
    "    modisco.tfmodisco_workflow.seqlets_to_patterns.TfModiscoSeqletsToPatternsFactory(\n",
    "        #Note: as of version 0.5.6.0, it's possible to use the results of a motif discovery\n",
    "        # software like MEME to improve the TF-MoDISco clustering. To use the meme-based\n",
    "        # initialization, you would specify the initclusterer_factory as shown in the\n",
    "        # commented-out code below:\n",
    "        initclusterer_factory=modisco.clusterinit.memeinit.MemeInitClustererFactory(   \n",
    "            meme_command=\"meme\", base_outdir=\"/data/scratch/rdeng/enhancer_project/ipython_notebooks/2D/contri_score/High/Note_Modisco/\",                     \n",
    "            max_num_seqlets_to_use=10000, nmotifs=10, n_jobs=1),\n",
    "        trim_to_window_size=21,\n",
    "        initial_flank_to_add=10,\n",
    "        final_flank_to_add=10,\n",
    "        final_min_cluster_size=60,\n",
    "        #use_pynnd=True can be used for faster nn comp at coarse grained step                       \n",
    "        # (it will use pynndescent), but note that pynndescent may crash               \n",
    "        #use_pynnd=True,\n",
    "        n_cores=10)\n",
    ")(\n",
    "    task_names=[\"task0\"],#, \"task1\", \"task2\"],\n",
    "    contrib_scores=contri_unpad,\n",
    "    hypothetical_contribs=hypoth_unpad,\n",
    "    one_hot=inps_unpad,\n",
    "    null_per_pos_scores=null_per_pos_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
